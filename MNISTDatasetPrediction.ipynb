{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnGsUSsQ5gop0dNlLeBcbN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SongitaD/AI/blob/main/MNISTDatasetPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR-I4vjYHck4",
        "outputId": "3ff15033-3644-4054-c875-4b09ea30cafa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data\n",
            "(60000, 28, 28)\n",
            "60000\n",
            "[5 0 4 ... 5 6 8]\n",
            "Test Data\n",
            "(10000, 28, 28)\n",
            "10000\n",
            "[7 2 1 ... 4 5 6]\n",
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.8760 - loss: 0.4401\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9670 - loss: 0.1144\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9788 - loss: 0.0713\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9843 - loss: 0.0518\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9888 - loss: 0.0371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78b171d102c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "[1.63154084e-07 1.05081421e-09 3.58466095e-05 1.64115321e-04\n",
            " 1.64185159e-11 1.03824206e-07 2.52288945e-12 9.99797583e-01\n",
            " 3.31605349e-07 1.87202784e-06]\n",
            "7\n",
            "0.9997976\n",
            "test_labels at 0 :  7\n",
            "[3.3879346e-07 4.9262740e-06 9.9995476e-01 2.6491512e-05 4.1516484e-13\n",
            " 4.0902199e-07 4.4644287e-07 5.3793585e-13 1.2545489e-05 3.1987111e-13]\n",
            "2\n",
            "0.99995476\n"
          ]
        }
      ],
      "source": [
        "# Working with MNIST dataset in Keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load MNIST dataset\n",
        "# train_images and train_labels are training set - data model learns from\n",
        "# test_images and test_model are testing data for model to be tested on\n",
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "print(\"Training data\")\n",
        "print(train_images.shape)\n",
        "print(len(train_labels))\n",
        "print(train_labels)\n",
        "\n",
        "print(\"Test Data\")\n",
        "print(test_images.shape)\n",
        "print(len(test_labels))\n",
        "print(test_labels)\n",
        "\n",
        "# 2 Dense layers\n",
        "model=keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")])\n",
        "\n",
        "# Compilation step of model\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Preparing the image data\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "# Fitting the model or train the model\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "\n",
        "# Use model to make predictions\n",
        "test_digits = test_images[0:10]\n",
        "predictions = model.predict(test_digits)\n",
        "print(predictions[0])\n",
        "print(predictions[0].argmax())\n",
        "print(predictions[0][7])\n",
        "print(\"test_labels at 0 : \", test_labels[0])\n",
        "print(predictions[1])\n",
        "print(predictions[1].argmax())\n",
        "index1=predictions[1].argmax()\n",
        "print(predictions[1][index1])\n",
        "\n",
        "\n"
      ]
    }
  ]
}